1. Scope of the Document

This document serves as the design reference and implementation plan for Feature 1241594 – Autonomy Log Database, owned by the AI Ecosystem – Data Pipeline team.

2. Overview
2.1 Problem Statement

1AHS team requires a way to query ECM log data based on attributes, support metadata tagging at ingest, and store metadata as a structured table linked to raw logs.

2.2 Objective

Create a new metadata table parallel to PTAG, ingest attributes via Airflow, expose via API, and ensure compatibility with future taxonomy-driven queries.

2.3 Timeline
Activity	Days	Estimated Completion
Design Doc	2	Week 1
Implementation – Schema	4	Week 2
Implementation – DAG/API	5	Week 3
Customer PoC	3	Week 4
Implementation – Polish	3	Week 5
Verification	2	Week 6
Documentation	1	Week 6
Sign Off	1	Week 6
3. Background

Current ECM pipeline stores logs with PTAG metadata. Integrity reports and annotations live in separate tables. Metadata tagging is limited, and PTAG is overloaded. The new attribute schema should sit parallel to PTAG without breaking existing queries.

4. Requirements

Ability to store structured metadata for log files at ingest.

Link metadata rows directly to raw log entries.

Support attribute key-value pairs similar to PTAG additional info, but structured.

Airflow-based ingest DAG to populate tables.

API endpoint to retrieve metadata via filters.

4.1 Use Cases

Query all logs where attribute weather = snow.

Metadata modified after ingest should trigger re-index in table.

PoC should allow uploading a JSON metadata sidecar alongside ECM logs.

4.2 User Interaction

User uploads log data + attribute JSON.

Airflow detects file, ingests metadata into table.

API allows GET /logs/search?attribute=....

4.3 Assumptions

ECM raw log table remains unchanged.

PTAG stays active temporarily; new metadata table runs parallel.

4.4 In-Scope

New table creation.

DAG ingestion.

API read endpoint.

4.5 Out-of-Scope

UI changes.

Backfill of historical logs.

PTAG deprecation.

5. Design
5.1 High-Level Design

A new log_metadata table will be created parallel to PTAG. Data ingestion happens through Airflow DAG triggered on new log uploads. Metadata is stored as row-per-attribute pair and linked via raw log ID.

5.2 Detailed Design

Flow:
File upload → JSON attributes detected → Airflow → Insert into log_metadata + link to raw_log.

A lightweight FastAPI endpoint exposes /logs/search.

5.3 Database/API Design

Tables to be added:

log_metadata_header – one row per log file.

log_metadata_attr – key-value attributes for each log.

API:

GET /log-metadata/search?key=&value=

5.4 Dependencies and Risks

Requires Airflow instance connectivity to database.

Requires agreement on metadata JSON schema.

Risk: Metadata JSON mismatch → ingestion failure.

5.5 Gaps

Decision pending: whether to enforce foreign keys or keep linkage loose.

Naming convention to align with PTAG not finalized.

Coordination with ECM pipeline for JSON location.

6. Testing
6.1 Unit Tests

Validate schema creation migration.

Validate JSON parse → DB rows.

6.2 Integration Tests

Full ingest of sample log + JSON → Query via API → Expect metadata.

7. Deployment

Code merged via feature branch → deployment via standard Airflow+API CICD.

Monitor first runs via DAG logs.

8. Work Estimate

Total engineering effort estimated at 5 sprints, aligned with feature tracking entry.

9. Evidence of Completion

Link to design doc: (this document)

Link to DAG run logs: [To be added]

Link to migration PR: [To be added]

10. References

ADO Feature 1241594

PTAG ERD source repo (internal)

Airflow ingestion pipeline spec (existing PTAG ingest code)

11. Notes

PTAG remains active until metadata table proves stable. Final migration plan will be handled separately.
