Task 1231269: â€œSubmit a compute job to AWSâ€

Hereâ€™s how you should proceed step by step:

ğŸ”¹ Step 1: Understand the workflows

The task says: â€œUnderstand all 3 compute workflows. Execute the manual submission.â€
From your earlier screenshots, the three workflows under Data Compute are:

Machine Learning (MLFlow / AWS Batch)

Log Processing (HPC-style / Metrics Pipeline)

CI Workflows (manual submission)

So, youâ€™ll need to show you can run at least one job manually on AWS, and describe the difference between them.

ğŸ”¹ Step 2: Set up environment

Clone the analytics repo (this is needed for metrics & ML pipelines):

git clone https://github.com/cat-autonomy/plat_aiesdp_analytics.git
cd plat_aiesdp_analytics/training_templates/metrics_pipeline/src


Create a Python venv & install requirements:

python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

ğŸ”¹ Step 3: Run a compute job (example: metrics pipeline)

Edit the experiment name in metrics_pipeline_run.py:

experiment_name = "metricsPipelineDemo_test"


Run the job:

python3 metrics_pipeline_run.py


This will generate metrics and push them to the metrics DB (via API).

You can then go to Superset (https://superset-dev.autonomy.cat.com/) â†’ Dashboard â†’ aiesdp_training to see results.

ğŸ”¹ Step 4: (Optional) Try MLFlow job

Follow the mlflow_template in the analytics repo.

Run a training job (e.g., linear regression on Iris dataset).

This pushes experiment results into MLFlow tracking.

ğŸ”¹ Step 5: Document in ADO

In the Discussion/Comments of the ADO task, write something like:

â€œCloned analytics repo, set up virtual env, installed dependencies.â€

â€œSubmitted metrics pipeline job (metrics_pipeline_run.py) â†’ metrics logged into DB â†’ verified in Superset.â€

â€œReviewed MLFlow and log-processing workflows; MLFlow uses AWS Batch, metrics pipeline logs metrics via API.â€

â€œNext: can extend to CI workflow submissions.â€
